{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wpny3sZQLwyc",
        "outputId": "2f32d49e-6891-43e3-d7d1-01d35de79b9f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "\n",
        "project_path = \"/content/drive/MyDrive/resume_ai_agent\"\n",
        "os.makedirs(project_path, exist_ok=True)\n",
        "\n",
        "print(f\"âœ… Project directory created at: {project_path}\")"
      ],
      "metadata": {
        "id": "n436NmENS-Da",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "765835a4-dd93-4600-8d63-41b684b8c3e3"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Project directory created at: /content/drive/MyDrive/resume_ai_agent\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "resume_reader_code = '''\n",
        "import fitz  # PyMuPDF\n",
        "\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    doc = fitz.open(pdf_path)\n",
        "    text = \"\"\n",
        "    for page in doc:\n",
        "        text += page.get_text()\n",
        "    return text\n",
        "'''\n",
        "\n",
        "with open(f\"{project_path}/resume_reader.py\", \"w\") as f:\n",
        "    f.write(resume_reader_code)\n",
        "\n",
        "print(\"âœ… resume_reader.py saved.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EBk71NQYcmIN",
        "outputId": "d287364a-93c4-46ee-f09e-1231859fb2b3"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… resume_reader.py saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install PyPDF2"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5LOdt-2ijwXh",
        "outputId": "4ff31485-5f9d-4d3f-bf2d-dcd7a1831a87"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting PyPDF2\n",
            "  Downloading pypdf2-3.0.1-py3-none-any.whl.metadata (6.8 kB)\n",
            "Downloading pypdf2-3.0.1-py3-none-any.whl (232 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m232.6/232.6 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: PyPDF2\n",
            "Successfully installed PyPDF2-3.0.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pip install beautifulsoup4 requests"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7KD9ERY6kSXF",
        "outputId": "37bf06ea-cc63-4d55-84c6-c8541e472269"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in /usr/local/lib/python3.11/dist-packages (4.13.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (2.32.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (2.6)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from beautifulsoup4) (4.13.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U0WOJMg4o-UM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import files\n",
        "\n",
        "uploaded = files.upload()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 74
        },
        "id": "WJ0zOc6Hlq2s",
        "outputId": "cfce712e-ba12-4f0a-9b63-c8f89eb28379"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-d5917cec-44f7-4639-85ab-29672a15190b\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-d5917cec-44f7-4639-85ab-29672a15190b\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving Yash_CV_06.pdf to Yash_CV_06.pdf\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wBlmBDYOeLtv"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from PyPDF2 import PdfReader\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "def identify_sources(jobs):\n",
        "    sources = set()\n",
        "    for job in jobs:\n",
        "        link = job.get(\"job_apply_link\", \"\")\n",
        "        domain = urlparse(link).netloc\n",
        "        if domain:\n",
        "            sources.add(domain)\n",
        "    print(\"ðŸ” Job sources found:\")\n",
        "    for s in sources:\n",
        "        print(\"â€¢\", s)\n",
        "\n",
        "\n",
        "# --- 1. Extract resume text ---\n",
        "def extract_resume_text(file_path):\n",
        "    reader = PdfReader(file_path)\n",
        "    return \" \".join(page.extract_text() for page in reader.pages)\n",
        "\n",
        "# --- 2. Extract keywords ---\n",
        "def extract_keywords(resume_text):\n",
        "    words = resume_text.lower().split()\n",
        "    keywords = [word.strip(\".,()\") for word in words if len(word) > 4]\n",
        "    return list(set(keywords))\n",
        "\n",
        "\n",
        "\n",
        "# --- 3. Search jobs from Indeed ---\n",
        "\n",
        "def search_jobs(user_keywords, location, query, country, language):\n",
        "    url = \"https://jsearch.p.rapidapi.com/search\"\n",
        "\n",
        "    combined_query = f\"{query} {' '.join(user_keywords)} {location}\".strip()\n",
        "\n",
        "    headers = {\n",
        "        \"X-RapidAPI-Key\": \"be5738a9e9msh08bfcd8fad98672p136d4bjsn7df8eb80c816\",\n",
        "        \"X-RapidAPI-Host\": \"jsearch.p.rapidapi.com\"\n",
        "    }\n",
        "\n",
        "    for c in country:\n",
        "        for l in language:\n",
        "            params = {\n",
        "            \"query\": combined_query,\n",
        "            \"page\": \"1\",\n",
        "            \"num_pages\": \"1\",\n",
        "            \"country\": c,         # e.g., 'us', 'in', etc.\n",
        "            \"language\": l        # e.g., 'en', 'hi', etc.\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                response = requests.get(url, headers=headers, params=params)\n",
        "                print(response.status_code)\n",
        "                print(response.text)\n",
        "                data = response.json().get(\"data\", [])\n",
        "            except Exception as e:\n",
        "                print(\"âŒ Error during API call:\", e)\n",
        "            return []\n",
        "\n",
        "        if not data:\n",
        "            print(\"âŒ No jobs found. Try different keywords or location.\\n\")\n",
        "            return []\n",
        "\n",
        "    print(f\"\\nðŸŽ¯ Jobs for: {combined_query}\\n\")\n",
        "    results = []\n",
        "    for job in data[:5]:  # top 5\n",
        "        print(f\"ðŸ“Œ {job['job_title']} at {job['employer_name']}\")\n",
        "        print(f\"ðŸŒ Location: {job.get('job_city', 'N/A')}, {job.get('job_country', 'N/A')}\")\n",
        "        print(f\"ðŸ”— {job['job_apply_link']}\\n\")\n",
        "        print(f\"ðŸ”— {job['job_apply_link']}\\n\")\n",
        "        results.append(f\"{job['job_title']} at {job['employer_name']} â€” {job['job_apply_link']}\")\n",
        "\n",
        "    return results\n",
        "\n",
        "# --- 4. Match jobs to resume ---\n",
        "def match_jobs(resume_keywords, jobs):\n",
        "    matches = []\n",
        "    for job in jobs:\n",
        "        score = sum(1 for kw in resume_keywords if kw in job.lower())\n",
        "        if score >= 1:  # adjust threshold\n",
        "            matches.append(job)\n",
        "    return matches\n",
        "\n",
        "# --- 5. Run everything ---\n",
        "def main():\n",
        "    resume_path = list(uploaded.keys())[0]  # first uploaded file\n",
        "    text = extract_resume_text(resume_path)\n",
        "    #keywords = extract_keywords(text)\n",
        "    keywords = [\n",
        "    \"Python\", \"Flask\", \"FastAPI\", \"PostgreSQL\", \"AWS Lambda\", \"Jenkins\", \"Visual Studio\",\n",
        "    \"Machine Learning\", \"Deep Learning\", \"KNN\", \"Random Forest\", \"Logistic Regression\",\n",
        "    \"Convolutional Neural Network\", \"Grid Search\", \"K-Fold\", \"MIT-BIH\", \"ECG\", \"SPSS\",\n",
        "    \"SWOT Analysis\", \"Immunization Systems\", \"Data Science\", \"Web Development\",\n",
        "    \"JavaScript\", \"HTML\", \"CSS\", \"MySQL\", \"DBMS\", \"Neural Networks\", \"OOPs\", \"Java\", \"Ubuntu\",\n",
        "    \"Prompt Engineering\", \"Business Analyst\", \"Amazon Web Services\"]\n",
        "\n",
        "    print(\"ðŸ” Extracted keywords:\", keywords[:10])\n",
        "\n",
        "    #jobs = search_jobs(keywords, \"data scientist\", \"remote\")\n",
        "    positions = [\"data scientist\", \"ml engineer\", \"ai developer\", \"software developer\", \"cybersecurity\"]\n",
        "    locations = [\"remote\", \"new york\", \"san francisco\", \"india\", \"lucknow\", \"mumbai\", \"bengaluru\"]\n",
        "    country = ['in', 'us', 'uk', 'ca', 'de']\n",
        "    language = ['en']#, 'ar', 'fr']\n",
        "\n",
        "    jobs = []\n",
        "    for position in positions:\n",
        "        for location in locations:\n",
        "            print(f\"\\nðŸ” Searching for: {position} in {location}\")\n",
        "            found_jobs = search_jobs(keywords, position, location, country, language)\n",
        "            if found_jobs:\n",
        "                jobs.extend(found_jobs)\n",
        "\n",
        "    identify_sources(jobs)\n",
        "    matched = match_jobs(keywords, jobs)\n",
        "\n",
        "    print(\"\\nðŸŽ¯ Top Matched Jobs:\\n\")\n",
        "    for job in matched:\n",
        "        print(\"â€¢\", job)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ8M3ch70OC8",
        "outputId": "ca602bb4-43a0-4eb9-f770-2c23b848c146"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "ðŸ” Extracted keywords: ['Python', 'Flask', 'FastAPI', 'PostgreSQL', 'AWS Lambda', 'Jenkins', 'Visual Studio', 'Machine Learning', 'Deep Learning', 'KNN']\n",
            "\n",
            "ðŸ” Searching for: data scientist in remote\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"73e71495-c777-49a4-9d3c-fe93016d4fe5\",\"parameters\":{\"query\":\"remote python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services data scientist\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: data scientist in new york\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"e4a9cffa-7327-4fc4-a6ef-a356b237a388\",\"parameters\":{\"query\":\"new york python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services data scientist\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: data scientist in san francisco\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"6421c959-0b3c-4a7c-aac8-eb80e9bab760\",\"parameters\":{\"query\":\"san francisco python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services data scientist\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: data scientist in india\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"476e6cb7-380f-451a-b7e3-a55e512f8b9a\",\"parameters\":{\"query\":\"india python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services data scientist\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: data scientist in lucknow\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"1a180e0e-6910-4127-b51f-ae9138b2b288\",\"parameters\":{\"query\":\"lucknow python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services data scientist\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: data scientist in mumbai\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"fdf74007-781e-4565-86ba-54bf0bb5f846\",\"parameters\":{\"query\":\"mumbai python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services data scientist\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: data scientist in bengaluru\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"f57ab8b7-59b1-4ac1-8ddc-fe21c4a15408\",\"parameters\":{\"query\":\"bengaluru python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services data scientist\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: ml engineer in remote\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"fb48bd03-b790-474f-a1c6-57112dc150af\",\"parameters\":{\"query\":\"remote python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services ml engineer\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: ml engineer in new york\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"679f73fc-f7bb-492d-8630-0a50b38eedec\",\"parameters\":{\"query\":\"new york python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services ml engineer\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: ml engineer in san francisco\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"5461cad7-764a-45a6-bc06-67acd24335ef\",\"parameters\":{\"query\":\"san francisco python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services ml engineer\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: ml engineer in india\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"43823082-baef-4306-81ce-5afe9b34b029\",\"parameters\":{\"query\":\"india python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services ml engineer\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: ml engineer in lucknow\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"5ef18f1e-ae86-40bf-b365-36a49383bd54\",\"parameters\":{\"query\":\"lucknow python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services ml engineer\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: ml engineer in mumbai\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"83f0aa52-af8e-4a00-9b8d-536937a930fc\",\"parameters\":{\"query\":\"mumbai python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services ml engineer\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: ml engineer in bengaluru\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"3de39225-918e-4d26-91f4-a6b645c7954a\",\"parameters\":{\"query\":\"bengaluru python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services ml engineer\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: ai developer in remote\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"65c5c867-b120-48a4-b575-fbf0feee2742\",\"parameters\":{\"query\":\"remote python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services ai developer\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: ai developer in new york\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"7d9abb25-db52-4208-a30c-07bb8578bded\",\"parameters\":{\"query\":\"new york python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services ai developer\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: ai developer in san francisco\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"204e455a-e341-419e-ab3d-b08cd549838b\",\"parameters\":{\"query\":\"san francisco python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services ai developer\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: ai developer in india\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"72bf6c30-aca0-463b-a953-4391d1aac1ae\",\"parameters\":{\"query\":\"india python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services ai developer\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: ai developer in lucknow\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"59f1a2ff-98c4-448c-a581-0c8664a1e4f4\",\"parameters\":{\"query\":\"lucknow python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services ai developer\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: ai developer in mumbai\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"4b22600a-d4c7-4999-819d-330b4b84dad6\",\"parameters\":{\"query\":\"mumbai python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services ai developer\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: ai developer in bengaluru\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"0e7004f0-300b-4cb8-b51b-4e5ee628d544\",\"parameters\":{\"query\":\"bengaluru python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services ai developer\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: software developer in remote\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"a654e8e2-02fb-46f9-a3f6-1b1b2b6d1cd3\",\"parameters\":{\"query\":\"remote python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services software developer\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: software developer in new york\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"b1506e1d-f4d6-48ec-b537-9e011718f64b\",\"parameters\":{\"query\":\"new york python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services software developer\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: software developer in san francisco\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"fa29e808-4690-4ace-8479-fbbec6af2c40\",\"parameters\":{\"query\":\"san francisco python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services software developer\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: software developer in india\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"7bee9994-12bf-467b-82c8-e63b476adead\",\"parameters\":{\"query\":\"india python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services software developer\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: software developer in lucknow\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"138e122a-7769-43da-9406-ba9270a35d66\",\"parameters\":{\"query\":\"lucknow python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services software developer\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: software developer in mumbai\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"1e124e31-1e6f-4e8d-b709-c0b5d17f3aa4\",\"parameters\":{\"query\":\"mumbai python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services software developer\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: software developer in bengaluru\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"d64f3e98-97ba-44cc-95bc-c0b5c8fcd639\",\"parameters\":{\"query\":\"bengaluru python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services software developer\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: cybersecurity in remote\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"7292679c-4074-48bc-8d90-9b708078f11f\",\"parameters\":{\"query\":\"remote python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services cybersecurity\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: cybersecurity in new york\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"47c5a39a-22a9-4b6f-acd1-43103b172b3f\",\"parameters\":{\"query\":\"new york python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services cybersecurity\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: cybersecurity in san francisco\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"51272f56-f810-4d55-a128-8ec75c564b64\",\"parameters\":{\"query\":\"san francisco python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services cybersecurity\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: cybersecurity in india\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"58e03649-2048-4984-9ef6-441af17380d1\",\"parameters\":{\"query\":\"india python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services cybersecurity\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: cybersecurity in lucknow\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"d4bac3d0-7c70-4437-ba95-7bb5467c98dd\",\"parameters\":{\"query\":\"lucknow python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services cybersecurity\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: cybersecurity in mumbai\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"2a2410fd-dcbd-4bc1-8fd2-bb0023676e2c\",\"parameters\":{\"query\":\"mumbai python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services cybersecurity\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "\n",
            "ðŸ” Searching for: cybersecurity in bengaluru\n",
            "200\n",
            "{\"status\":\"OK\",\"request_id\":\"0a3c8d41-8c63-465f-9f46-04533f6e58c2\",\"parameters\":{\"query\":\"bengaluru python flask fastapi postgresql aws lambda jenkins visual studio machine learning deep learning knn random forest logistic regression convolutional neural network grid search k-fold mit-bih ecg spss swot analysis immunization systems data science web development javascript html css mysql dbms neural networks oops java ubuntu prompt engineering business analyst amazon web services cybersecurity\",\"page\":1,\"num_pages\":1,\"country\":\"in\",\"language\":\"en\"},\"data\":[]}\n",
            "ðŸ” Job sources found:\n",
            "\n",
            "ðŸŽ¯ Top Matched Jobs:\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from PyPDF2 import PdfReader\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "def identify_sources(jobs):\n",
        "    sources = set()\n",
        "    for job in jobs:\n",
        "        link = job.get(\"job_apply_link\", \"\")\n",
        "        domain = urlparse(link).netloc\n",
        "        if domain:\n",
        "            sources.add(domain)\n",
        "    print(\"ðŸ” Job sources found:\")\n",
        "    for s in sources:\n",
        "        print(\"â€¢\", s)\n",
        "\n",
        "\n",
        "# --- 1. Extract resume text ---\n",
        "def extract_resume_text(file_path):\n",
        "    reader = PdfReader(file_path)\n",
        "    return \" \".join(page.extract_text() for page in reader.pages)\n",
        "\n",
        "# --- 2. Extract keywords ---\n",
        "def extract_keywords(resume_text):\n",
        "    words = resume_text.lower().split()\n",
        "    keywords = [word.strip(\".,()\") for word in words if len(word) > 4]\n",
        "    return list(set(keywords))\n",
        "\n",
        "\n",
        "\n",
        "# --- 3. Search jobs from Indeed ---\n",
        "\"\"\"\n",
        "def search_jobs(user_keywords, position, location, countries, languages):\n",
        "    url = \"https://jsearch.p.rapidapi.com/search\"\n",
        "    headers = {\n",
        "        \"X-RapidAPI-Key\": \"cb9cac225amshf0ccd5867ef7942p159e93jsncd4b0385f515\",\n",
        "        \"X-RapidAPI-Host\": \"jsearch.p.rapidapi.com\"\n",
        "    }\n",
        "\n",
        "    results = []\n",
        "\n",
        "    for c in countries:\n",
        "        for l in languages:\n",
        "            query = f\"{position} {location} {' '.join(user_keywords[:5])}\"  # fewer keywords = better matching\n",
        "            params = {\n",
        "                \"query\": query,\n",
        "                \"page\": \"1\",\n",
        "                \"num_pages\": \"1\",\n",
        "                \"country\": c,\n",
        "                \"language\": l\n",
        "            }\n",
        "\n",
        "            response = requests.get(url, headers=headers, params=params)\n",
        "            print(response.status_code)\n",
        "            print(response.text)\n",
        "            jobs = response.json().get(\"data\", [])\n",
        "\n",
        "            for job in jobs:\n",
        "                results.append({\n",
        "                    'title': job.get('job_title', ''),\n",
        "                    'company': job.get('employer_name', ''),\n",
        "                    'link': job.get('job_apply_link', ''),\n",
        "                    'location': f\"{job.get('job_city', 'N/A')}, {job.get('job_country', 'N/A')}\"\n",
        "                })\n",
        "\n",
        "    return results\"\"\"\n",
        "\n",
        "def search_jobs(user_keywords, location, query, country, language):\n",
        "    url = \"https://jsearch.p.rapidapi.com/search\"\n",
        "    combined_query = f\"{query} {' '.join(user_keywords)} {location}\".strip()\n",
        "\n",
        "    headers = {\n",
        "        \"X-RapidAPI-Key\": \"cb9cac225amshf0ccd5867ef7942p159e93jsncd4b0385f515\",\n",
        "        \"X-RapidAPI-Host\": \"jsearch.p.rapidapi.com\"\n",
        "    }\n",
        "\n",
        "    all_data = []  # â† collect results here\n",
        "\n",
        "    for c in country:\n",
        "        for l in language:\n",
        "            params = {\n",
        "                \"query\": combined_query,\n",
        "                \"page\": \"1\",\n",
        "                \"num_pages\": \"1\",\n",
        "                \"country\": c,\n",
        "                \"language\": l\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                response = requests.get(url, headers=headers, params=params)\n",
        "                print(\"âœ… STATUS:\", response.status_code)   # <--- This will now show 200 if successful\n",
        "                print(\"ðŸ“¨ RAW RESPONSE:\", response.text[:500])  # <--- Print first 500 chars\n",
        "                data = response.json().get(\"data\", [])\n",
        "                all_data.extend(data)\n",
        "            except Exception as e:\n",
        "                print(\"âŒ API ERROR:\", e)\n",
        "\n",
        "    return all_data\n",
        "\n",
        "\n",
        "# --- 4. Match jobs to resume ---\n",
        "def match_jobs(resume_keywords, jobs):\n",
        "    matches = []\n",
        "    for job in jobs:\n",
        "        score = sum(1 for kw in resume_keywords if kw in job.lower())\n",
        "        if score >= 1:  # adjust threshold\n",
        "            matches.append(job)\n",
        "    return matches\n",
        "\n",
        "# --- 5. Run everything ---\n",
        "def main():\n",
        "    positions = [\"Data Scientist\", \"ML Engineer\"]\n",
        "    locations = [\"Remote\", \"India\"]\n",
        "    countries = [\"us\", \"in\"]\n",
        "    languages = [\"en\"]\n",
        "\n",
        "    keywords = [\"Python\", \"Machine Learning\", \"AWS\", \"SQL\"]  # clean, sharp keywords\n",
        "\n",
        "    jobs = []\n",
        "    for position in positions:\n",
        "        for location in locations:\n",
        "            print(f\"\\nðŸ” Searching: {position} in {location}\")\n",
        "            jobs.extend(search_jobs(keywords, position, location, countries, languages))\n",
        "\n",
        "    print(f\"\\nâœ… Total Jobs Found: {len(jobs)}\\n\")\n",
        "    for job in jobs[:5]:\n",
        "        print(f\"ðŸ“Œ {job['title']} at {job['company']} | {job['location']}\")\n",
        "        print(f\"ðŸ”— {job['link']}\\n\")"
      ],
      "metadata": {
        "id": "nWmWUtOtMea-"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "from PyPDF2 import PdfReader\n",
        "from urllib.parse import urlparse\n",
        "\n",
        "def identify_sources(jobs):\n",
        "    sources = set()\n",
        "    for job in jobs:\n",
        "        link = job.get(\"job_apply_link\", \"\")\n",
        "        domain = urlparse(link).netloc\n",
        "        if domain:\n",
        "            sources.add(domain)\n",
        "    print(\"ðŸ” Job sources found:\")\n",
        "    for s in sources:\n",
        "        print(\"â€¢\", s)\n",
        "\n",
        "\n",
        "# --- 1. Extract resume text ---\n",
        "def extract_resume_text(file_path):\n",
        "    reader = PdfReader(file_path)\n",
        "    return \" \".join(page.extract_text() for page in reader.pages)\n",
        "\n",
        "# --- 2. Extract keywords ---\n",
        "def extract_keywords(resume_text):\n",
        "    words = resume_text.lower().split()\n",
        "    keywords = [word.strip(\".,()\") for word in words if len(word) > 4]\n",
        "    return list(set(keywords))\n",
        "\n",
        "\n",
        "\n",
        "# --- 3. Search jobs from Indeed ---\n",
        "\n",
        "def search_jobs(user_keywords, location, query, country, language):\n",
        "    print(\"obs.extend(search_jobs(keywords, position, location, countries, languages))\")\n",
        "    url = \"https://jsearch.p.rapidapi.com/search\"\n",
        "    combined_query = f\"{query} {' '.join(user_keywords)} {location}\".strip()\n",
        "\n",
        "    headers = {\n",
        "        \"X-RapidAPI-Key\": \"cb9cac225amshf0ccd5867ef7942p159e93jsncd4b0385f515\",\n",
        "        \"X-RapidAPI-Host\": \"jsearch.p.rapidapi.com\"\n",
        "    }\n",
        "\n",
        "    all_data = []  # â† collect results here\n",
        "\n",
        "    for c in country:\n",
        "        for l in language:\n",
        "            params = {\n",
        "                \"query\": combined_query,\n",
        "                \"page\": \"1\",\n",
        "                \"num_pages\": \"1\",\n",
        "                \"country\": c,\n",
        "                \"language\": l\n",
        "            }\n",
        "\n",
        "            try:\n",
        "                response = requests.get(url, headers=headers, params=params)\n",
        "                print(response)\n",
        "                print(\"âœ… STATUS:\", response.status_code)   # <--- This will now show 200 if successful\n",
        "                print(\"ðŸ“¨ RAW RESPONSE:\", response.text[:500])  # <--- Print first 500 chars\n",
        "                data = response.json().get(\"data\", [])\n",
        "                print(data)\n",
        "                all_data.extend(data)\n",
        "            except Exception as e:\n",
        "                print(\"âŒ API ERROR:\", e)\n",
        "\n",
        "    return all_data\n",
        "\n",
        "\n",
        "# --- 4. Match jobs to resume ---\n",
        "def match_jobs(resume_keywords, jobs):\n",
        "    matches = []\n",
        "    for job in jobs:\n",
        "        score = sum(1 for kw in resume_keywords if kw in job.lower())\n",
        "        if score >= 1:  # adjust threshold\n",
        "            matches.append(job)\n",
        "    return matches\n",
        "\n",
        "# --- 5. Run everything ---\n",
        "def main():\n",
        "\n",
        "    positions = [\"Data Scientist\"]\n",
        "    locations = [\"Remote\"]\n",
        "    countries = [\"us\"]\n",
        "    languages = [\"en\"]\n",
        "\n",
        "\n",
        "    keywords = [\"Python\", \"ML\", \"SQL\"]\n",
        "    print(\"start\\n\")\n",
        "    jobs = []\n",
        "    for position in positions:\n",
        "        for location in locations:\n",
        "            print(f\"\\nðŸ” Searching: {position} in {location}\")\n",
        "            jobs.extend(search_jobs(keywords, position, location, countries, languages))\n",
        "\n",
        "    print(f\"\\nâœ… Total Jobs Found: {len(jobs)}\\n\")\n",
        "    for job in jobs[:5]:\n",
        "        if job.get('job_title') or job.get('employer_name') or job.get('job_city') or job.get('job_country'):\n",
        "            print(\"ðŸ“Œ\", job.get('job_title', 'N/A'), \"at\", job.get('employer_name', 'N/A'))\n",
        "            print(\"ðŸ“\", job.get('job_city', 'N/A'), \",\", job.get('job_country', 'N/A'))\n",
        "            print(\"ðŸ”—\", job.get('job_apply_link', 'N/A'), \"\\n\")\n",
        "    print(\"end\\n\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1Ewug3wrj91N",
        "outputId": "719ddb04-dbf8-4ebc-ce6f-19dff706a872"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start\n",
            "\n",
            "\n",
            "ðŸ” Searching: Data Scientist in Remote\n",
            "obs.extend(search_jobs(keywords, position, location, countries, languages))\n",
            "<Response [200]>\n",
            "âœ… STATUS: 200\n",
            "ðŸ“¨ RAW RESPONSE: {\"status\":\"OK\",\"request_id\":\"64510cab-6725-40a8-8cbe-7b29726d899a\",\"parameters\":{\"query\":\"remote python ml sql data scientist\",\"page\":1,\"num_pages\":1,\"country\":\"us\",\"language\":\"en\"},\"data\":[{\"job_id\":\"ZfSglzo2plh7nKH3AAAAAA==\",\"job_title\":\"Data Scientist - Python (US Remote)\",\"employer_name\":\"KnowBe4\",\"employer_logo\":\"https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTNpp3Oek7lFmeT0H7t6hyq16n_YJ-C-s-2qRX4&s=0\",\"employer_website\":\"https://www.knowbe4.com\",\"job_publisher\":\"EdTech Jobs\",\"job_em\n",
            "[{'job_id': 'ZfSglzo2plh7nKH3AAAAAA==', 'job_title': 'Data Scientist - Python (US Remote)', 'employer_name': 'KnowBe4', 'employer_logo': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTNpp3Oek7lFmeT0H7t6hyq16n_YJ-C-s-2qRX4&s=0', 'employer_website': 'https://www.knowbe4.com', 'job_publisher': 'EdTech Jobs', 'job_employment_type': 'Full-time', 'job_employment_types': ['FULLTIME'], 'job_apply_link': 'https://edtechjobs.io/jobs/116676503-data-scientist-python-us-remote?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'job_apply_is_direct': False, 'apply_options': [{'publisher': 'EdTech Jobs', 'apply_link': 'https://edtechjobs.io/jobs/116676503-data-scientist-python-us-remote?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': False}, {'publisher': 'Aijobs.net', 'apply_link': 'https://aijobs.net/job/1045935-data-scientist-python-us-remote/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': False}, {'publisher': 'LikeRemote', 'apply_link': 'https://likeremote.com/remote-jobs/knowbe4-remote-job-data-scientist-python-us-remote--911228?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': False}], 'job_description': \"Fast Facts\\n\\nWe are seeking a Remote Data Scientist with strong Python skills to collaborate with business stakeholders, develop algorithms, and interpret complex data insights to enhance operational efficiency.\\n\\nResponsibilities: Key responsibilities include designing data modeling processes, implementing machine learning algorithms, and effectively communicating data insights to stakeholders.\\n\\nSkills: Required skills include advanced proficiency in Python, R, SQL, and data science libraries; strong foundation in statistics; and experience with DataOps and MLOps best practices.\\n\\nQualifications: Preferred qualifications include a BS or equivalent with 3 years of experience, and an MS/Ph.D. is a plus but not required.\\n\\nLocation: This is a fully remote position open only to candidates located in the US.\\n\\nCompensation: $100000 - $120000 / Annually\\n\\nRemote positions open to the US only.\\n\\nData scientists work closely with business stakeholders to understand their goals and identify data-driven strategies to achieve those goals. They design data modeling processes, create algorithms and predictive models to extract the insight the business needs and help analyze the data to increase the productivity and efficiency of the business.\\n\\nResponsibilities:\\nâ€¢ Expertise working experience with programming languages like Python, R, and SQL\\nâ€¢ Solid understanding of statistics, probability, and machine learning\\nâ€¢ Research, design, and implement Machine Learning algorithms to solve complex problems\\nâ€¢ Communicate complex concepts and statistical models to non-technical audiences through data visualizations\\n\\nRequired Skills:\\nâ€¢ BS or equivalent plus 3 years experience\\nâ€¢ MS/Ph.D. or equivalent; no experience required\\nâ€¢ Expert-level SQL proficiency with experience in Snowflake, dbt, and Looker\\nâ€¢ Advanced Python programming with demonstrated expertise in data science libraries (numpy, pandas, matplotlib, scikit-learn)\\nâ€¢ Strong foundation in statistics and mathematical principles underlying data science algorithms\\nâ€¢ Extensive experience in developing and deploying machine learning models, including\\nâ€¢ Feature engineering and selection, Model training, validation, and testing, Sampling and data reduction techniques\\nâ€¢ Experience in establishing and implementing DataOps and MLOps best practices\\nâ€¢ Demonstrated ability to quantify and communicate business impact through data-driven insights\\nâ€¢ Track record of identifying and leading high-impact data science initiatives\\nâ€¢ Experience in building and maintaining stakeholder relationships across all organizational levels\\nâ€¢ Excellent communication skills, particularly in explaining technical concepts to non-technical audiences\\n\\nThe base pay for this position ranges from $100,000 - $120,000, which will vary depending on how well an applicant's skills and experience align with the job description listed above.\\n\\nWe will accept applications until 5/25/2025.\", 'job_is_remote': True, 'job_posted_at': '22 days ago', 'job_posted_at_timestamp': 1741737600, 'job_posted_at_datetime_utc': '2025-03-12T00:00:00.000Z', 'job_location': None, 'job_city': None, 'job_state': None, 'job_country': None, 'job_latitude': None, 'job_longitude': None, 'job_benefits': None, 'job_google_link': 'https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DZfSglzo2plh7nKH3AAAAAA%3D%3D&vssid=jobs-detail-viewer', 'job_min_salary': 100000, 'job_max_salary': 120000, 'job_salary_period': 'YEAR', 'job_highlights': {}, 'job_onet_soc': '15111100', 'job_onet_job_zone': '5'}, {'job_id': 'MidFbm2vsJU8jXpmAAAAAA==', 'job_title': 'Data Scientist-12199-Remote', 'employer_name': 'Shuvel Digital', 'employer_logo': None, 'employer_website': 'https://www.shuvel.net', 'job_publisher': 'Monster', 'job_employment_type': 'Full-time', 'job_employment_types': ['FULLTIME'], 'job_apply_link': 'https://www.monster.com/job-openings/data-scientist-12199-remote--f63c8af7-b314-428a-ae2a-b854d6c29956?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'job_apply_is_direct': False, 'apply_options': [{'publisher': 'Monster', 'apply_link': 'https://www.monster.com/job-openings/data-scientist-12199-remote--f63c8af7-b314-428a-ae2a-b854d6c29956?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': False}, {'publisher': 'Shuvel Digital', 'apply_link': 'https://shuvel.breezy.hr/p/b62958673ba8-data-scientist-12199-remote?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': False}, {'publisher': 'Jobgether', 'apply_link': 'https://jobgether.com/offer/67a12820b327e4f74cc67832-data-scientist-12199-remote-at-shuvel-digital?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': False}, {'publisher': 'Jobs', 'apply_link': 'https://sw-siemens.dejobs.org/virtual-usa/data-scientist-12199-remote/FD83E73F40654C8DB977CA05AA57CDCB/job/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': False}, {'publisher': 'Emplois Chez SCI - Service Corporation International', 'apply_link': 'https://emplois.sci-corp.com/virtual-usa/data-scientist-12199-remote/FD83E73F40654C8DB977CA05AA57CDCB/job/?vs=28&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': False}, {'publisher': 'Who Doo U Node.com', 'apply_link': 'https://www.whodoounode.com/employment/data-scientist-12199-remote-listing-35282.aspx?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': False}, {'publisher': 'Jobrapido.com', 'apply_link': 'https://us.jobrapido.com/jobpreview/2463928768125730816?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': False}], 'job_description': \"Responsibility:\\nâ€¢ Build and enhance Non-SQL/SQL based data analytics processes through all phases of development, implementation, and upkeep.\\nâ€¢ Unlock insights by analyzing large scale of complex numerical and textual data and identifying trends.\\nâ€¢ Create and maintain reporting dashboards to deliver data insights to a wide range of consumers.\\nâ€¢ Partner with a cross-functional team of data engineers, machine learning engineers, and data visualizers to deliver projects.\\nâ€¢ Develop data science solutions utilizing both on-premises and Microsoft Azure Cloud Computing Infrastructures.\\nâ€¢ Perform other duties as assigned.\\n\\nQualifications:\\nâ€¢ Bachelor's degree in Computer Science, Mathematics, Statistics, or a related field (or equivalent work experience).\\nâ€¢ Experience with cloud computing infrastructure.\\nâ€¢ Minimum of 5 years of experience in Data Analysis, SQL/Python Programming, and Plotly Enterprise or Power BI development.\\nâ€¢ Strong understanding of database design, data mining, and data modeling concepts.\\nâ€¢ Experience with ETL processes and data integration techniques.\\nâ€¢ Proven ability to develop effective data visualizations using Power BI and/or Plotly Enterprise.\\nâ€¢ Advanced verbal, written, interpersonal, and presentation skills to communicate clearly and concisely technical and non-technical information to all levels of management.\\n\\nDesired:\\nâ€¢ Advanced degree in in computer science, mathematics, physics, statistics, or related field.\\nâ€¢ Advanced skills with Python, Jupyter Notebook/Jupyter Lab, Visual Studio Code and other languages/frameworks appropriate for large data analysis.\\nâ€¢ Experience with supervised/unsupervised machine learning model development in areas of clustering, classification and/or regression.\\nâ€¢ Experience with No-SQL databases such as MongoDB and Cassandra\\nâ€¢ Experience with API end point connection and configuration for complex data pipelines.\\nâ€¢ Experience establishing data governance principals as part of the data ingestion process.\\nâ€¢ Experience working with Agile principals utilizing Azure DevOps or similar systems.\\nâ€¢ Experience working in the Financial Industry\\n\\nRemote\\n\\nSkills:\\nAgile Programming Methodologies, Apache Cassandra, Application Programming Interface (API), Cloud Computing, Communication Skills, Computer Science, Computer Skills, Cross-Functional, Data Analysis, Data Management, Data Mining, Data Modeling, Data Processing, Data Science, Data Visualization, Database Design, Database Extract Transform and Load (ETL), DevOps, Finance, Interpersonal Skills, Laboratory Notebook, Machine Learning, Mathematics, Microsoft Visual Studio, Microsoft Windows Azure, MongoDB, NoSQL, Physics, Power BI, Presentation/Verbal Skills, Python Programming/Scripting Language, Reporting Dashboards, SQL (Structured Query Language), SQL Databases, Statistics, Trend Analysis, Writing Skills\\n\\nAbout the Company:\\nShuvel Digital\", 'job_is_remote': True, 'job_posted_at': None, 'job_posted_at_timestamp': None, 'job_posted_at_datetime_utc': None, 'job_location': 'United States', 'job_city': None, 'job_state': None, 'job_country': 'US', 'job_latitude': 38.794595199999996, 'job_longitude': -106.5348379, 'job_benefits': None, 'job_google_link': 'https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DMidFbm2vsJU8jXpmAAAAAA%3D%3D&vssid=jobs-detail-viewer', 'job_salary': None, 'job_min_salary': None, 'job_max_salary': None, 'job_salary_period': None, 'job_highlights': {'Qualifications': [\"Bachelor's degree in Computer Science, Mathematics, Statistics, or a related field (or equivalent work experience)\", 'Experience with cloud computing infrastructure', 'Minimum of 5 years of experience in Data Analysis, SQL/Python Programming, and Plotly Enterprise or Power BI development', 'Strong understanding of database design, data mining, and data modeling concepts', 'Experience with ETL processes and data integration techniques', 'Proven ability to develop effective data visualizations using Power BI and/or Plotly Enterprise', 'Advanced verbal, written, interpersonal, and presentation skills to communicate clearly and concisely technical and non-technical information to all levels of management', 'Agile Programming Methodologies, Apache Cassandra, Application Programming Interface (API), Cloud Computing, Communication Skills, Computer Science, Computer Skills, Cross-Functional, Data Analysis, Data Management, Data Mining, Data Modeling, Data Processing, Data Science, Data Visualization, Database Design, Database Extract Transform and Load (ETL), DevOps, Finance, Interpersonal Skills, Laboratory Notebook, Machine Learning, Mathematics, Microsoft Visual Studio, Microsoft Windows Azure, MongoDB, NoSQL, Physics, Power BI, Presentation/Verbal Skills, Python Programming/Scripting Language, Reporting Dashboards, SQL (Structured Query Language), SQL Databases, Statistics, Trend Analysis, Writing Skills'], 'Responsibilities': ['Build and enhance Non-SQL/SQL based data analytics processes through all phases of development, implementation, and upkeep', 'Unlock insights by analyzing large scale of complex numerical and textual data and identifying trends', 'Create and maintain reporting dashboards to deliver data insights to a wide range of consumers', 'Partner with a cross-functional team of data engineers, machine learning engineers, and data visualizers to deliver projects', 'Develop data science solutions utilizing both on-premises and Microsoft Azure Cloud Computing Infrastructures', 'Perform other duties as assigned']}, 'job_onet_soc': '15111100', 'job_onet_job_zone': '5'}, {'job_id': 'kF8KbgFBVBkX7nLHAAAAAA==', 'job_title': 'Data Scientist IV Main - Medicare, Medicaid, Claims, SQL, SAS, Python', 'employer_name': 'Kaiser Permanente', 'employer_logo': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQ1geHz-_1YxTXtTa_Hu0AZOdX_hkjWBRyPsCcx&s=0', 'employer_website': 'https://healthy.kaiserpermanente.org', 'job_publisher': 'Dice', 'job_employment_type': 'Full-time', 'job_employment_types': ['FULLTIME'], 'job_apply_link': 'https://www.dice.com/job-detail/b7722d47-8544-4a2e-9915-f1d0ecaa402a?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'job_apply_is_direct': False, 'apply_options': [{'publisher': 'Dice', 'apply_link': 'https://www.dice.com/job-detail/b7722d47-8544-4a2e-9915-f1d0ecaa402a?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': False}], 'job_description': \"Description: Job Summary:\\n\\nThis individual contributor is primarily responsible for designing and developing data pipelines and automation for data acquisition and ingestion of raw data from multiple data sources and data formats by transforming, cleansing, and storing data for consumption. This role is also responsible for developing detailed problem statements outlining hypotheses and their effect on target clients/customers, analyzing and investigating complex data sets and summarizing key characteristics, selecting, manipulating and transforming data into features used in machine learning algorithms, training statistical models, deploying and maintaining reliable and efficient models through production, verifying model performance, and collaborating with internal and external stakeholders across domains to develop and deliver statistical driven outcomes.\\nEssential Responsibilities:\\nâ€¢ Promotes learning in others by proactively providing and/or developing information, resources, advice, and expertise with coworkers and members; builds relationships with cross-functional/external stakeholders and customers. Listens to, seeks, and addresses performance feedback; proactively provides actionable feedback to others and to managers. Pursues self-development; creates and executes plans to capitalize on strengths and develop weaknesses; leads by influencing others through technical explanations and examples and provides options and recommendations. Adopts new responsibilities; adapts to and learns from change, challenges, and feedback; demonstrates flexibility in approaches to work; champions change and helps others adapt to new tasks and processes. Facilitates team collaboration to support a business outcome.\\nâ€¢ Completes work assignments autonomously and supports business-specific projects by applying expertise in subject area and business knowledge to generate creative solutions; encourages team members to adapt to and follow all procedures and policies. Collaborates cross-functionally and/or externally to achieve effective business decisions; provides recommendations and solves complex problems; escalates high-priority issues or risks, as appropriate; monitors progress and results. Supports the development of work plans to meet business priorities and deadlines; identifies resources to accomplish priorities and deadlines. Identifies, speaks up, and capitalizes on improvement opportunities across teams; uses influence to guide others and engages stakeholders to achieve appropriate solutions.\\nâ€¢ Develops detailed problem statements outlining hypotheses and their effect on target clients/customers by defining scope, objectives, outcome statements and metrics.\\nâ€¢ Designs and develops data pipelines and automation for data acquisition and ingestion of raw data from multiple data sources and data formats by transforming, cleansing, and storing data for consumption by downstream processes; writing and optimizing diverse SQL queries; and demonstrating advanced knowledge of database fundamentals.\\nâ€¢ Analyzes and investigates complex data sets and summarizes key characteristics by employing data visualization methods; and determining how best to manipulate data sources to discover patterns, spot anomalies, test hypotheses, and/or check assumptions.\\nâ€¢ Selects, manipulates, and transforms data into features used in machine learning algorithms by leveraging techniques to conduct dimensionality reduction, feature importance, and feature selection.\\nâ€¢ Trains statistical models by using algorithms and data mining techniques; testing models with various algorithms to assess the input dataset and related features; and applying techniques to prevent overfitting such as cross-validation.\\nâ€¢ Deploys and maintains reliable and efficient models through production.\\nâ€¢ Verifies model performance by demonstrating expertise in the practice of a variety of model validation techniques to assess and discriminate the goodness of model fit; and leveraging feedback and output to manage and strengthen model performance.\\nâ€¢ Collaborates with internal and external stakeholders across domains to develop and deliver statistical driven outcomes by delivering insights and values from heterogeneous data to investigate complex problems for multiple use cases; driving informed decision-making; and presenting findings to both technical and non-technical audiences.\\nMinimum Qualifications:\\nâ€¢ Minimum three (3) years experience working with Exploratory Data Analysis (EDA) and visualization methods.\\nâ€¢ Minimum three (3) years machine learning and/or algorithmic experience.\\nâ€¢ Minimum three (3) years statistical analysis and modeling experience.\\nâ€¢ Minimum three (3) years programming experience.\\nâ€¢ Minimum one (1) year experience in a leadership role with or without direct reports.\\nâ€¢ Bachelors degree in Mathematics, Statistics, Computer Science, Engineering, Economics, Public Health, or related field AND Minimum five (5) years experience in data science or a directly related field. Additional equivalent work experience in a directly related field may be substituted for the degree requirement. Advanced degrees may be substituted for the work experience requirements.\\n\\nAdditional Requirements:\\nâ€¢ Knowledge, Skills, and Abilities (KSAs): Advanced Quantitative Data Modeling; Algorithms; Applied Data Analysis; Data Extraction; Data Visualization Tools; Machine Learning; Relational Database Management; Microsoft Excel; Design Thinking; Business Intelligence Tools; Data Manipulation/Wrangling; Data Ensemble Techniques; Feature Analysis/Engineering; Open Source Languages & Tools; Model Optimization; Strategic Thinking; Deep Learning/Neural Networks; Project Management\\nPreferred Qualifications:\\nâ€¢ One (1) year experience working with Kubernetes.\\nâ€¢ One (1) year experience working with Docker.\\n\\nPrimary Location: California,Oakland,Ordway\\nScheduled Weekly Hours: 40\\nShift: Day\\nWorkdays: Mon, Tue, Wed, Thu, Fri\\nWorking Hours Start: 08:00 AM\\nWorking Hours End: 05:00 PM\\nJob Schedule: Full-time\\nJob Type: Standard\\nWorker Location: Remote\\nEmployee Status: Regular\\nEmployee Group/Union Affiliation: NUE-PO-01|NUE|Non Union Employee\\nJob Level: Individual Contributor\\nSpecialty: Data Science\\nDepartment: Po/Ho Corp - Medicare LOB Admin - 0308\\nPay Range: $166100 - $214940 / year Kaiser Permanente strives to offer a market competitive total rewards package and is committed to pay equity and transparency. The posted pay range is based on possible base salaries for the role and does not reflect the full value of our total rewards package. Actual base pay determined at offer will be based on labor market data and a candidate's years of relevant work experience, education, certifications, skills, and geographic location.\\nTravel: No\\nRemote: Work location is the remote workplace (from home) within KP authorized states. Worker location must align with Kaiser Permanente's Authorized States policy. At Kaiser Permanente, equity, inclusion and diversity are inextricably linked to our mission, and we aim to make it a part of everything we do. We know that having a diverse and inclusive workforce makes Kaiser Permanente a better place to receive health care, a more supportive partner in our communities we serve, and a more fulfilling place to work. Working at Kaiser Permanente means that you agree to and abide by our commitment to equity and our expectation that we all work together to create an inclusive work environment focused on a sense of belonging and wellbeing.\\n\\nKaiser Permanente is an equal opportunity employer committed to a diverse and inclusive workforce. Applicants will receive consideration for employment without regard to race, color, religion, sex (including pregnancy), age, sexual orientation, national origin, marital status, parental status, ancestry, disability, gender identity, veteran status, genetic information, other distinguishing characteristics of diversity and inclusion, or any other protected status. Submit Interest\", 'job_is_remote': True, 'job_posted_at': '11 hours ago', 'job_posted_at_timestamp': 1743624000, 'job_posted_at_datetime_utc': '2025-04-02T20:00:00.000Z', 'job_location': 'Oakland, CA', 'job_city': 'Oakland', 'job_state': 'California', 'job_country': 'US', 'job_latitude': 37.8043514, 'job_longitude': -122.27116389999999, 'job_benefits': None, 'job_google_link': 'https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DkF8KbgFBVBkX7nLHAAAAAA%3D%3D&vssid=jobs-detail-viewer', 'job_min_salary': None, 'job_max_salary': None, 'job_salary_period': 'YEAR', 'job_highlights': {'Qualifications': ['Minimum three (3) years experience working with Exploratory Data Analysis (EDA) and visualization methods', 'Minimum three (3) years machine learning and/or algorithmic experience', 'Minimum three (3) years statistical analysis and modeling experience', 'Minimum three (3) years programming experience', 'Minimum one (1) year experience in a leadership role with or without direct reports', 'Bachelors degree in Mathematics, Statistics, Computer Science, Engineering, Economics, Public Health, or related field AND Minimum five (5) years experience in data science or a directly related field', 'Additional equivalent work experience in a directly related field may be substituted for the degree requirement', 'Advanced degrees may be substituted for the work experience requirements', 'Knowledge, Skills, and Abilities (KSAs): Advanced Quantitative Data Modeling; Algorithms; Applied Data Analysis; Data Extraction; Data Visualization Tools; Machine Learning; Relational Database Management; Microsoft Excel; Design Thinking; Business Intelligence Tools; Data Manipulation/Wrangling; Data Ensemble Techniques; Feature Analysis/Engineering; Open Source Languages & Tools; Model Optimization; Strategic Thinking; Deep Learning/Neural Networks; Project Management', \"Worker location must align with Kaiser Permanente's Authorized States policy\"], 'Benefits': ['Scheduled Weekly Hours: 40', 'Job Schedule: Full-time', 'Pay Range: $166100 - $214940 / year Kaiser Permanente strives to offer a market competitive total rewards package and is committed to pay equity and transparency', 'The posted pay range is based on possible base salaries for the role and does not reflect the full value of our total rewards package', \"Actual base pay determined at offer will be based on labor market data and a candidate's years of relevant work experience, education, certifications, skills, and geographic location\"], 'Responsibilities': ['This individual contributor is primarily responsible for designing and developing data pipelines and automation for data acquisition and ingestion of raw data from multiple data sources and data formats by transforming, cleansing, and storing data for consumption', 'This role is also responsible for developing detailed problem statements outlining hypotheses and their effect on target clients/customers, analyzing and investigating complex data sets and summarizing key characteristics, selecting, manipulating and transforming data into features used in machine learning algorithms, training statistical models, deploying and maintaining reliable and efficient models through production, verifying model performance, and collaborating with internal and external stakeholders across domains to develop and deliver statistical driven outcomes', 'Promotes learning in others by proactively providing and/or developing information, resources, advice, and expertise with coworkers and members; builds relationships with cross-functional/external stakeholders and customers', 'Listens to, seeks, and addresses performance feedback; proactively provides actionable feedback to others and to managers', 'Pursues self-development; creates and executes plans to capitalize on strengths and develop weaknesses; leads by influencing others through technical explanations and examples and provides options and recommendations', 'Adopts new responsibilities; adapts to and learns from change, challenges, and feedback; demonstrates flexibility in approaches to work; champions change and helps others adapt to new tasks and processes', 'Facilitates team collaboration to support a business outcome', 'Completes work assignments autonomously and supports business-specific projects by applying expertise in subject area and business knowledge to generate creative solutions; encourages team members to adapt to and follow all procedures and policies', 'Collaborates cross-functionally and/or externally to achieve effective business decisions; provides recommendations and solves complex problems; escalates high-priority issues or risks, as appropriate; monitors progress and results', 'Supports the development of work plans to meet business priorities and deadlines; identifies resources to accomplish priorities and deadlines', 'Identifies, speaks up, and capitalizes on improvement opportunities across teams; uses influence to guide others and engages stakeholders to achieve appropriate solutions', 'Develops detailed problem statements outlining hypotheses and their effect on target clients/customers by defining scope, objectives, outcome statements and metrics', 'Designs and develops data pipelines and automation for data acquisition and ingestion of raw data from multiple data sources and data formats by transforming, cleansing, and storing data for consumption by downstream processes; writing and optimizing diverse SQL queries; and demonstrating advanced knowledge of database fundamentals', 'Analyzes and investigates complex data sets and summarizes key characteristics by employing data visualization methods; and determining how best to manipulate data sources to discover patterns, spot anomalies, test hypotheses, and/or check assumptions', 'Selects, manipulates, and transforms data into features used in machine learning algorithms by leveraging techniques to conduct dimensionality reduction, feature importance, and feature selection', 'Trains statistical models by using algorithms and data mining techniques; testing models with various algorithms to assess the input dataset and related features; and applying techniques to prevent overfitting such as cross-validation', 'Deploys and maintains reliable and efficient models through production', 'Verifies model performance by demonstrating expertise in the practice of a variety of model validation techniques to assess and discriminate the goodness of model fit; and leveraging feedback and output to manage and strengthen model performance', 'Collaborates with internal and external stakeholders across domains to develop and deliver statistical driven outcomes by delivering insights and values from heterogeneous data to investigate complex problems for multiple use cases; driving informed decision-making; and presenting findings to both technical and non-technical audiences', 'Workdays: Mon, Tue, Wed, Thu, Fri', 'Working Hours Start: 08:00 AM', 'Working Hours End: 05:00 PM']}, 'job_onet_soc': '15204100', 'job_onet_job_zone': '5'}, {'job_id': 't7Rhk08jfB-8C38uAAAAAA==', 'job_title': 'Data Scientist - Digital and Service Operations (Work from home) - Evernorth', 'employer_name': 'Cigna', 'employer_logo': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcT_6bjWAjRu8I3Q3gZ9X1n_wj99zAagvV_rN6-W&s=0', 'employer_website': 'https://www.cigna.com', 'job_publisher': 'Karkidi', 'job_employment_type': 'Full-time', 'job_employment_types': ['FULLTIME'], 'job_apply_link': 'https://www.karkidi.com/job-details/7244-data-scientist-digital-and-service-operations-work-from-home-evernorth-job?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'job_apply_is_direct': False, 'apply_options': [{'publisher': 'Karkidi', 'apply_link': 'https://www.karkidi.com/job-details/7244-data-scientist-digital-and-service-operations-work-from-home-evernorth-job?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': False}], 'job_description': \"Cigna is seeking a Data Scientist who is passionate about machine learning, building models, data mining and NLP in their Service Operations group. This is a work from home position for candidates based in the US.\\n\\nThe Position\\nThe Data Scientist position is a hands-on role responsible for the end-to-end design, testing and development of machine learning models for Service Operations. As a Data Scientist in GD&A you will work closely with other team members and business stakeholders to apply machine learning, deep learning, NLP and other advanced analytic techniques to solve business problems. You will research and prepare data for analysis, perform data exploration, apply machine learning and statistical techniques to draw conclusions and build innovative analytic solutions to improve operational effectiveness and efficiency.\\n\\nThis position is with Evernorth, a new business within the Cigna Corporation.\\n\\nResponsibilities\\nâ€¢ Work individually and/or with team members to design and develop machine learning models for prototyping or ad-hoc analyses.\\nâ€¢ Apply data mining techniques and perform statistical analysis as needed.\\nâ€¢ Collect and organize information from a variety of data sources. Write queries to underlying Microsoft SQL Server and Oracle databases.\\nâ€¢ Work with business partners to learn and understand the specific domain (i.e. Claim Ops, Call/Contact Center, Client Servicesâ€¦).\\nâ€¢ Interview stakeholders in order to understand pain points.\\nâ€¢ Clearly present findings to leadership and be able to articulate and influence the Business Case for change.\\n\\nQualifications\\nâ€¢ Minimum 5 years of hands-on, industry experience in machine learning algorithms and statistical modeling techniques such as clustering, classification, regression, decision trees, neural nets, support vector machines, ensemble modeling and text mining techniques such as sentiment analysis, topic modeling and entity extraction.\\nâ€¢ Multi-year experience with Python and SQL.\\nâ€¢ Exceptional quantitative skills and attention to detail.\\nâ€¢ Predictive modeling experience preferred.\\nâ€¢ Experience in claims preferred.\\nâ€¢ Degree in Data Science, Computer Science, Statistics, Mathematics, Computational Linguistics or related field, strongly preferred. MS or PhD preferred.\\nâ€¢ Ability to establish and maintain strong working relationships across the organization.\\n\\nThis position is not eligible to be performed in Colorado.\\n\\nAbout Evernorth\\n\\nEvernorth, Cigna Corporationâ€™s health services segment, exists to elevate health for all. We're building on our legacy and redefining health care as we know it. Unbiased in how we think, we create without limitation. We partner without constraints, deliver value differently and act in the interest of humanity. Solving across silos, closing gaps in care, and empowering clients, customers, and people everywhere to move onward and upward. When you work with us, youâ€™ll be empowered to solve the problems others donâ€™t, wonâ€™t or canâ€™t. Join us. What difference will you make?\", 'job_is_remote': True, 'job_posted_at': None, 'job_posted_at_timestamp': None, 'job_posted_at_datetime_utc': None, 'job_location': 'United States', 'job_city': None, 'job_state': None, 'job_country': 'US', 'job_latitude': 38.794595199999996, 'job_longitude': -106.5348379, 'job_benefits': None, 'job_google_link': 'https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Dt7Rhk08jfB-8C38uAAAAAA%3D%3D&vssid=jobs-detail-viewer', 'job_min_salary': 120000, 'job_max_salary': 190000, 'job_salary_period': 'YEAR', 'job_highlights': {'Qualifications': ['Minimum 5 years of hands-on, industry experience in machine learning algorithms and statistical modeling techniques such as clustering, classification, regression, decision trees, neural nets, support vector machines, ensemble modeling and text mining techniques such as sentiment analysis, topic modeling and entity extraction', 'Multi-year experience with Python and SQL', 'Exceptional quantitative skills and attention to detail'], 'Responsibilities': ['The Data Scientist position is a hands-on role responsible for the end-to-end design, testing and development of machine learning models for Service Operations', 'As a Data Scientist in GD&A you will work closely with other team members and business stakeholders to apply machine learning, deep learning, NLP and other advanced analytic techniques to solve business problems', 'You will research and prepare data for analysis, perform data exploration, apply machine learning and statistical techniques to draw conclusions and build innovative analytic solutions to improve operational effectiveness and efficiency', 'Work individually and/or with team members to design and develop machine learning models for prototyping or ad-hoc analyses', 'Apply data mining techniques and perform statistical analysis as needed', 'Collect and organize information from a variety of data sources', 'Write queries to underlying Microsoft SQL Server and Oracle databases', 'Work with business partners to learn and understand the specific domain (i.e', 'Claim Ops, Call/Contact Center, Client Servicesâ€¦)', 'Interview stakeholders in order to understand pain points', 'Clearly present findings to leadership and be able to articulate and influence the Business Case for change']}, 'job_onet_soc': '15204100', 'job_onet_job_zone': '5'}, {'job_id': 'y9Kfwt0prv9aIKlEAAAAAA==', 'job_title': 'AI Data Engineer', 'employer_name': 'Aquent', 'employer_logo': None, 'employer_website': 'https://aquent.com', 'job_publisher': 'Aquent', 'job_employment_type': 'Full-time and Contractor', 'job_employment_types': ['FULLTIME', 'CONTRACTOR', 'CONTRACTOR'], 'job_apply_link': 'https://aquent.com/find-work/200379?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'job_apply_is_direct': False, 'apply_options': [{'publisher': 'Aquent', 'apply_link': 'https://aquent.com/find-work/200379?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': False}, {'publisher': 'Indeed', 'apply_link': 'https://www.indeed.com/viewjob?jk=6933644b37e59bed&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': False}, {'publisher': 'Aquent Talent', 'apply_link': 'https://aquenttalent.com/talent/jobs/200379?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': False}, {'publisher': 'ZipRecruiter', 'apply_link': 'https://www.ziprecruiter.com/c/RIT-Solutions/Job/Azure-Gen-AI-Data-Engineer/-in-Arlington,VA?jid=dee5f585d17ed5ee&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': False}, {'publisher': 'Jobs', 'apply_link': 'https://jobs.ashbyhq.com/replit/08da7f58-b792-41cc-b6f0-adb47ebf0bee/application?utm_source=devignite.io&utm_medium=devignite.io&utm_campaign=devignite.io&ref=devignite.io&source=devignite.io&lever-origin=applied&lever-source%5B%5D=devignite.io&lever-requisition-name=devignite.io&lever-posting-owner-name=devignite.io&lever-source=devignite.io&lever-referer=devignite.io&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': False}, {'publisher': 'Teleperformance', 'apply_link': 'https://careersus-teleperformance.icims.com/jobs/62181/ai-data-engineer/job?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': True}, {'publisher': 'Glassdoor', 'apply_link': 'https://www.glassdoor.com/job-listing/ai-data-engineer-limestone-digital-JV_KO0,16_KE17,34.htm?jl=1009650093166&utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': False}, {'publisher': 'Jobgether', 'apply_link': 'https://jobgether.com/offer/67dcac5306b3a152e5129036-ai-data-operations-labeling-engineer?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': False}], 'job_description': 'Job Title: AI Data Engineer\\nLocation: On-site in Moline, Ill preferred but may consider Remote in the U.S. (CST. hours required)\\nJob Type: Full-Time Hourly\\nWe are looking for an AI Data Engineer to join our clientâ€™s team and contribute to the development of AI-driven systems using large-scale data solutions.\\nJob Overview:\\nWe are seeking a highly skilled and motivated AI Data Engineer with experience in Open AI, Python, PySpark, Langchain, large data sets, AWS, and expertise in SQL queries.\\nThis role will involve working on the design, development, and maintenance of robust data pipelines, facilitating the integration of AI models, and processing large-scale datasets in cloud-based environments.\\nKey Responsibilities:\\n\\nData Engineering: Design, develop, and manage data pipelines to handle and process large datasets with a focus on scalability and efficiency.\\nLangchain Integration: Utilize Langchain to build and optimize AI workflows, automate processes, and enhance AI model capabilities with data-driven features.\\nCloud Data Infrastructure: Manage and scale data infrastructure on AWS, using services such as S3, EC2, Lambda, Redshift, and others.\\nSQL & Database Expertise: Write complex SQL queries for data extraction, transformation, and analysis. Optimize database performance and ensure data integrity.\\nCollaboration: Work closely with Data Scientists, AI Engineers, and other stakeholders to ensure data is properly structured for machine learning models and other analytics use cases.\\nData Quality & Governance: Monitor data quality, create processes for error handling, and ensure compliance with data privacy regulations and best practices.\\nPerformance Optimization: Continuously optimize and fine-tune data processing pipelines and database queries for better performance and scalability.\\nDocumentation: Maintain detailed documentation on the architecture, data processes, and code to ensure smooth collaboration and knowledge transfer.\\n\\nRequirements:\\n\\nExperience:\\n\\n3+ years of experience as a Data Engineer, with a focus on AI or machine learning data pipelines.\\nHands-on experience with Langchain for building and optimizing AI workflows and automation.\\nStrong experience with large-scale data management, including working with distributed systems and processing massive datasets.\\nProficient in SQL and experience with relational databases (e.g., PostgreSQL, MySQL, SQL Server) and NoSQL databases.\\nExtensive experience with AWS\\n\\nTechnical Skills:\\n\\nProficient in Python and PySpark\\nStrong understanding of database design, data modeling, and query optimization.\\nFamiliarity with data warehousing concepts and tools.\\nExperience working with data pipelines and workflow orchestration tools like Apache Airflow, or similar.\\nKnowledge of AI and machine learning concepts, particularly around data preprocessing and feature engineering.\\n\\nEducation:\\n\\nBachelorâ€™s or Masterâ€™s degree in Computer Science, Data Engineering, Artificial Intelligence, or a related field.\\n\\nSoft Skills:\\n\\nStrong problem-solving and analytical skills.\\nExcellent communication skills, with the ability to explain technical concepts to non-technical stakeholders.\\nAbility to work in a fast-paced environment and manage multiple tasks effectively.\\nTeam-oriented with a collaborative attitude.\\n\\nBenefits:\\n\\nSubsidized Health, dental, and vision insurance.\\n\\nIf youâ€™re passionate about data engineering, AI, and cloud-based technologies, and you thrive in an innovative and dynamic environment, weâ€™d love to hear from you! Apply today and help us shape the future of AI-driven solutions.', 'job_is_remote': True, 'job_posted_at': '29 days ago', 'job_posted_at_timestamp': 1741132800, 'job_posted_at_datetime_utc': '2025-03-05T00:00:00.000Z', 'job_location': 'United States', 'job_city': None, 'job_state': None, 'job_country': 'US', 'job_latitude': 38.794595199999996, 'job_longitude': -106.5348379, 'job_benefits': ['dental_coverage', 'health_insurance'], 'job_google_link': 'https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Dy9Kfwt0prv9aIKlEAAAAAA%3D%3D&vssid=jobs-detail-viewer', 'job_salary': None, 'job_min_salary': None, 'job_max_salary': None, 'job_salary_period': None, 'job_highlights': {'Qualifications': ['3+ years of experience as a Data Engineer, with a focus on AI or machine learning data pipelines', 'Hands-on experience with Langchain for building and optimizing AI workflows and automation', 'Strong experience with large-scale data management, including working with distributed systems and processing massive datasets', 'Proficient in SQL and experience with relational databases (e.g., PostgreSQL, MySQL, SQL Server) and NoSQL databases', 'Extensive experience with AWS', 'Proficient in Python and PySpark', 'Strong understanding of database design, data modeling, and query optimization', 'Familiarity with data warehousing concepts and tools', 'Experience working with data pipelines and workflow orchestration tools like Apache Airflow, or similar', 'Knowledge of AI and machine learning concepts, particularly around data preprocessing and feature engineering', 'Bachelorâ€™s or Masterâ€™s degree in Computer Science, Data Engineering, Artificial Intelligence, or a related field', 'Strong problem-solving and analytical skills', 'Excellent communication skills, with the ability to explain technical concepts to non-technical stakeholders', 'Ability to work in a fast-paced environment and manage multiple tasks effectively', 'Team-oriented with a collaborative attitude'], 'Benefits': ['Subsidized Health, dental, and vision insurance'], 'Responsibilities': ['This role will involve working on the design, development, and maintenance of robust data pipelines, facilitating the integration of AI models, and processing large-scale datasets in cloud-based environments', 'Data Engineering: Design, develop, and manage data pipelines to handle and process large datasets with a focus on scalability and efficiency', 'Langchain Integration: Utilize Langchain to build and optimize AI workflows, automate processes, and enhance AI model capabilities with data-driven features', 'Cloud Data Infrastructure: Manage and scale data infrastructure on AWS, using services such as S3, EC2, Lambda, Redshift, and others', 'SQL & Database Expertise: Write complex SQL queries for data extraction, transformation, and analysis', 'Optimize database performance and ensure data integrity', 'Collaboration: Work closely with Data Scientists, AI Engineers, and other stakeholders to ensure data is properly structured for machine learning models and other analytics use cases', 'Data Quality & Governance: Monitor data quality, create processes for error handling, and ensure compliance with data privacy regulations and best practices', 'Performance Optimization: Continuously optimize and fine-tune data processing pipelines and database queries for better performance and scalability', 'Documentation: Maintain detailed documentation on the architecture, data processes, and code to ensure smooth collaboration and knowledge transfer']}, 'job_onet_soc': '15111100', 'job_onet_job_zone': '5'}, {'job_id': 'cJVcjy72LA7O12pPAAAAAA==', 'job_title': 'HEOR Scientist (Real-World Data Analytics/SAS/SQL/Python)', 'employer_name': 'Systems Ally', 'employer_logo': None, 'employer_website': None, 'job_publisher': 'SimplyHired', 'job_employment_type': 'Contractor', 'job_employment_types': ['CONTRACTOR', 'CONTRACTOR'], 'job_apply_link': 'https://www.simplyhired.com/job/YKoNHe3nJcT__kaEqB9LcTSKBWeFHuF-XJQ-9G0OJT9osdYzlFGsRw?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'job_apply_is_direct': False, 'apply_options': [{'publisher': 'SimplyHired', 'apply_link': 'https://www.simplyhired.com/job/YKoNHe3nJcT__kaEqB9LcTSKBWeFHuF-XJQ-9G0OJT9osdYzlFGsRw?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': False}], 'job_description': \"Remote\\n\\nDescription\\nâ€¢ As a Real-World Data (RWD) Scientist , you will be part of a team powering data driven insights that lead to better, faster decisions and therapies for our patients.\\nâ€¢ Acting as a lead data scientist in your project area, you will contribute RWD expertise to broader cross-functional initiatives.\\nâ€¢ Candidates should be excited to drive innovation by implementing new business technology solutions that solve significant scientific or business problems through the use and understanding of complex data.\\nâ€¢ Your role will be to conceive, design and execute analytical components of research studies using sources of Real-World Data and Genetics including, but not limited to large healthcare administrative databases, electronic medical records, registries and surveys.\\nâ€¢ Your expertise will be critical as you investigate, identify, develop, and optimize new methods, algorithms, and technologies to derive novel, competitive insights from disparate data sources.\\n\\nMajor Responsibilities:\\nâ€¢ Conceive, design and implement new RWD business technology solutions that solve significant scientific or business problems through the integration, visualization, and analysis of large and complex data.\\nâ€¢ Demonstrate high proficiency across a wide range of technologies related to the integration, visualization, and analysis of large and complex real-world data sets.\\nâ€¢ Maintain broad expertise analyzing large real-world data including medical claims data, electronic medical records, survey data, etc.\\nâ€¢ Demonstrate the ability to resolve key project hurdles and assumptions by effectively utilizing available information and technical expertise.\\nâ€¢ Expand advanced methodology and adopt new technology capabilities such as machine learning, RWE dashboards and visualizations, automation, etc.\\nâ€¢ Utilize knowledge of the pharmaceutical and healthcare business in the rapid advancement of agile, impactful, and cost-effective solutions\\nâ€¢ Drive productivity and efficiency gains throughout multiple business areas.\\nâ€¢ Highly autonomous and productive in performing activities, requiring only minimal direction from or interaction with supervisor.\\nâ€¢ Proactively seek out new information and technologies in the literature/public domain and incorporate into individual project(s) as well as the overall program\\nâ€¢ Understand and adhere to corporate standards regarding applicable Corporate and Divisional Policies, including code of conduct, safety, GxP compliance, and data security.\\n\\nQualifications:\\nâ€¢ M.S. (Master of Science), or PhD with 2 years of experience in HEOR/Epidemiology or related area.\\nâ€¢ Background in life sciences or work experience in the pharmaceutical industry preferred.\\nâ€¢ Significant experience with SAS, SAS Macro SQL, or other programming for real-world data analytics (ie: R or Python)\\nâ€¢ Experience and/or training in the application of advanced scientific and analytical methods\\nâ€¢ Proven implementation of creative technology solutions that advanced the business\\nâ€¢ Excellent written and oral English communication skills\\n\\nJob Type: Contract\\n\\nPay: $58.00 - $60.00 per hour\\n\\nExpected hours: 40 per week\\n\\nBenefits:\\nâ€¢ Health insurance\\n\\nSchedule:\\nâ€¢ 8 hour shift\\nâ€¢ Day shift\\nâ€¢ Monday to Friday\\n\\nEducation:\\nâ€¢ Bachelor's (Preferred)\\n\\nExperience:\\nâ€¢ HEOR/Epidemiology background: 5 years (Preferred)\\nâ€¢ SAS: 5 years (Preferred)\\nâ€¢ R programming: 5 years (Preferred)\\nâ€¢ Python: 5 years (Preferred)\\nâ€¢ SQL: 5 years (Preferred)\\nâ€¢ pharma or biotech industry: 5 years (Preferred)\\nâ€¢ analyzing large RWD including medical claims data: 5 years (Preferred)\\n\\nWork Location: Remote\", 'job_is_remote': True, 'job_posted_at': '15 days ago', 'job_posted_at_timestamp': 1742342400, 'job_posted_at_datetime_utc': '2025-03-19T00:00:00.000Z', 'job_location': None, 'job_city': None, 'job_state': None, 'job_country': None, 'job_latitude': None, 'job_longitude': None, 'job_benefits': None, 'job_google_link': 'https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DcJVcjy72LA7O12pPAAAAAA%3D%3D&vssid=jobs-detail-viewer', 'job_salary': None, 'job_min_salary': None, 'job_max_salary': None, 'job_salary_period': None, 'job_highlights': {}, 'job_onet_soc': '19301100', 'job_onet_job_zone': '5'}, {'job_id': 'GUZmHshUwh8Zz1Z0AAAAAA==', 'job_title': 'Sr Data Scientist (Python/SQL/A/B Testing) [Remote]', 'employer_name': 'Braintrust', 'employer_logo': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTE08ThhIOxYF4ya9QU8nK6BJoQyt2_hgfV2HoF&s=0', 'employer_website': 'https://www.usebraintrust.com', 'job_publisher': 'Karkidi', 'job_employment_type': 'Full-time', 'job_employment_types': ['FULLTIME'], 'job_apply_link': 'https://www.karkidi.com/job-details/36860-sr-data-scientist-python-sql-a-b-testing-remote-job?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'job_apply_is_direct': False, 'apply_options': [{'publisher': 'Karkidi', 'apply_link': 'https://www.karkidi.com/job-details/36860-sr-data-scientist-python-sql-a-b-testing-remote-job?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': False}], 'job_description': \"Braintrust is the only network that gives in-demand talent all the freedom of freelance with all the benefits, community and stability of a full-time role. As the first decentralized talent network, our revolutionary Web3 model ensures the community that relies on Braintrust to find work are the same people who own and build it through the blockchain token, BTRST. So unlike other marketplaces that take 20% to 50% of talent earnings, Braintrust allows talent to keep 100% of earnings and to vote on key changes to improve the network. Braintrust is working to change the way freelance works â€“ for good.\\nâ€¢ JOB TYPE: Freelance, Contract Position (no agencies/C2C - see notes below)\\nâ€¢ LOCATION: Remote - United States only (TimeZone: PST/CIST | Partial overlap)\\nâ€¢ HOURLY RANGE: Our client is looking to pay $110 â€“ $135/hr\\nâ€¢ ESTIMATED DURATION: 40h/week - Long term\\n\\nTHE OPPORTUNITY\\n\\nRequirements\\n\\nTech 3 Skills: python, SQL, & data analysis.\\n\\n3 Soft Skills: ab testing experience, communication & presentation\\n\\nWhat youâ€™ll be working on\\n\\nDo you have a passion for learning through experiments? Do you love helping teams find the next big â€˜ahaâ€™ insight which changes the way a business thinks about its customers? Would you like to help drive a culture of experimentation across a global SaaS company?\\n\\nOur client is seeking world-class analytics and data science professionals to join their Experimentation Center of Excellence (ECOE) team. They're looking for people with business instincts, team mindset, creativity, and analytics, who are passionate about applying their skills to help teams learn from experiments.\\n\\nIn the ECOE team, you'll get to:\\nâ€¢ Identify ways to enable analysts and data scientists to experiment faster and more rigorously than today. Put in place tables and dashboards to speed the experimentation process.\\nâ€¢ Measure the impact of their experimentation center of excellence strategy; designing and analyzing the launch of experimentation tools and frameworks.\\nâ€¢ Assess the current state of experimentation through analyses and needs assessments.\\nâ€¢ Share your high-quality insights and recommendations to peers and leadership in order to influence or drive faster business decisions\\nâ€¢ Foster a world-class analytics culture, through education and the creation of self-service tools, to make a lasting change in how your analysts use experiments to make decisions\\n\\nOn your first day, we expect you to have:\\nâ€¢ Experience applying your analytics skills to contribute to projects or programs which have had a proven impact on business performance and process\\nâ€¢ An agile development mindset, appreciating the benefit of constant iteration and improvement. A very high bar for output quality, while balancing â€œhaving something now vs. perfection in the future\\nâ€¢ Expertise in data manipulation and some history with statistical programming languages (SQL/Python/R).\\n\\nMastery of telling stories with data and proficiency in visualizations tools (e.g. Tableau, Redash)\\n\\nStrong ability to communicate. Explaining complex concepts to diverse audiences, and crafting compelling stories.\", 'job_is_remote': True, 'job_posted_at': None, 'job_posted_at_timestamp': None, 'job_posted_at_datetime_utc': None, 'job_location': 'San Francisco, CA', 'job_city': 'San Francisco', 'job_state': 'California', 'job_country': 'US', 'job_latitude': 37.7749295, 'job_longitude': -122.4194155, 'job_benefits': None, 'job_google_link': 'https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DGUZmHshUwh8Zz1Z0AAAAAA%3D%3D&vssid=jobs-detail-viewer', 'job_min_salary': 120000, 'job_max_salary': 190000, 'job_salary_period': 'YEAR', 'job_highlights': {'Qualifications': ['Tech 3 Skills: python, SQL, & data analysis', '3 Soft Skills: ab testing experience, communication & presentation', \"They're looking for people with business instincts, team mindset, creativity, and analytics, who are passionate about applying their skills to help teams learn from experiments\", 'Experience applying your analytics skills to contribute to projects or programs which have had a proven impact on business performance and process', 'An agile development mindset, appreciating the benefit of constant iteration and improvement', 'A very high bar for output quality, while balancing â€œhaving something now vs. perfection in the future', 'Expertise in data manipulation and some history with statistical programming languages (SQL/Python/R)', 'Mastery of telling stories with data and proficiency in visualizations tools (e.g', 'Tableau, Redash)', 'Strong ability to communicate', 'Explaining complex concepts to diverse audiences, and crafting compelling stories'], 'Benefits': ['HOURLY RANGE: Our client is looking to pay $110 â€“ $135/hr', 'ESTIMATED DURATION: 40h/week - Long term'], 'Responsibilities': ['LOCATION: Remote - United States only (TimeZone: PST/CIST | Partial overlap)', 'Identify ways to enable analysts and data scientists to experiment faster and more rigorously than today', 'Put in place tables and dashboards to speed the experimentation process', 'Measure the impact of their experimentation center of excellence strategy; designing and analyzing the launch of experimentation tools and frameworks', 'Assess the current state of experimentation through analyses and needs assessments', 'Share your high-quality insights and recommendations to peers and leadership in order to influence or drive faster business decisions', 'Foster a world-class analytics culture, through education and the creation of self-service tools, to make a lasting change in how your analysts use experiments to make decisions']}, 'job_onet_soc': '15114100', 'job_onet_job_zone': '4'}, {'job_id': 'ejQOhmusCXDS9DGBAAAAAA==', 'job_title': 'Data Scientist - Work from Home', 'employer_name': 'Lumen Tech, Inc.', 'employer_logo': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcQZxqWfCgj9ZwrJvoP15Xm1Hsr2feq10z78RzES&s=0', 'employer_website': None, 'job_publisher': 'Karkidi', 'job_employment_type': 'Full-time', 'job_employment_types': ['FULLTIME'], 'job_apply_link': 'https://www.karkidi.com/job-details/33328-data-scientist-work-from-home-job?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'job_apply_is_direct': False, 'apply_options': [{'publisher': 'Karkidi', 'apply_link': 'https://www.karkidi.com/job-details/33328-data-scientist-work-from-home-job?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': False}, {'publisher': 'TieTalent', 'apply_link': 'https://tietalent.com/en/jobs/p-948324/united-states-data-scientist-work-from-home?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': False}, {'publisher': 'Mysmartpros', 'apply_link': 'https://www.mysmartpros.com/tuition/job/associate-data-scientist-remote-virtual-work-from-home-job/?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': False}, {'publisher': 'Thinkhumbly.com', 'apply_link': 'https://thinkhumbly.com/job/208786?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': False}, {'publisher': 'Recruiter Jobs', 'apply_link': 'https://jobs.recruiter.com/jobs/18771489205-data-scientist-work-from-home?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': False}, {'publisher': 'JoPilot', 'apply_link': 'https://jobs.jopilot.net/job/57A2u5UB_CyF9o_0u8VL0325?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': False}], 'job_description': \"A Senior Data Scientist develops tools to collect, clean, analyze and manage the data used by Mass Markets Marketing team. Employs techniques and theories drawn from mathematics, statistics, from the subdomains of machine learning, classification, cluster analysis, and data mining. Collaborates with cross-functional teams to understand complex business problems, assists on the build of statistical models and segmentation, and creates reports.\\n\\nThe Main Responsibilities\\n\\nWork as a member of a Data Science & Analytics team in Mass Markets Marketing to help transform the business by identifying opportunities for operational efficiencies across the organization. This will include:\\nâ€¢ Clearly communicating methods and conclusions with simple language and innovative visualizations to management and executives.\\nâ€¢ Working effectively on a team and as a self-driven individual contributor who researches and develops cutting-edge and effective solutions for the business.\\nâ€¢ Providing expertise in analyzing and visualizing large, complex, and unique data sets.\\nâ€¢ Creating new insights that will drive business growth and profitability in the short and long term.\\nâ€¢ Deploying analytics solutions in a continuous integration environment incorporating version control, build servers, and test servers.\\nâ€¢ Designing and implementing programs and automation to collect, mash up, process and analyze distributed real-world data.\\nâ€¢ The display of strong analytical thinking and skills to rapidly adopt new methods, tools, and programming languages, as needed.\\n\\nWhat We Look For in a Candidate\\n\\nRequired:\\nâ€¢ Bachelorâ€™s Degree with 4+ years of related experience\\nâ€¢ Expert SQL skills and experience with relational databases. Microsoft SQL Server and / or Oracle experience preferred.\\nâ€¢ Knowledge of segmentation modeling, predictive analytics, and report generation.\\nâ€¢ Programming experience with two or more analytics platforms, such as: Python, R, SAS, SPPS, Scala, Java, Perl, Mathematica or C++.\\nâ€¢ Familiarity with BI reporting tools (specifically Tableau, Power BI)\\nâ€¢ Iterate and innovate on generating novel solutions that help drive business decisions\\nâ€¢ General knowledge of ETL functions\\n\\nPreferred:\\nâ€¢ Master's or PhD Degree(s) in Operations Research, Math, Statistics, Computer Science, Physics, Analytics, or Economics with 2+ years of experience.\\nâ€¢ 6+ years of experience in statistical data analysis and predictive modeling.\\nâ€¢ Experience with SQL jobs, functions and stored procedures, import / export knowledge, general SQL tuning, etc.\\nâ€¢ Windows batch file creation and maintenance experience.\\n\\nRequisition\\n\\nWhen applying for a position, you may be subject to a background screen (criminal records check, motor vehicle report, and/or drug screen), depending on the requirements for the position. Job-related concerns noted in the background screen may disqualify you from the new position or your current role. Background results will be evaluated on a case-by-case basis.\", 'job_is_remote': True, 'job_posted_at': None, 'job_posted_at_timestamp': None, 'job_posted_at_datetime_utc': None, 'job_location': 'United States', 'job_city': None, 'job_state': None, 'job_country': 'US', 'job_latitude': 38.794595199999996, 'job_longitude': -106.5348379, 'job_benefits': None, 'job_google_link': 'https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DejQOhmusCXDS9DGBAAAAAA%3D%3D&vssid=jobs-detail-viewer', 'job_min_salary': 120000, 'job_max_salary': 190000, 'job_salary_period': 'YEAR', 'job_highlights': {'Qualifications': ['Bachelorâ€™s Degree with 4+ years of related experience', 'Expert SQL skills and experience with relational databases'], 'Responsibilities': ['A Senior Data Scientist develops tools to collect, clean, analyze and manage the data used by Mass Markets Marketing team', 'Employs techniques and theories drawn from mathematics, statistics, from the subdomains of machine learning, classification, cluster analysis, and data mining', 'Collaborates with cross-functional teams to understand complex business problems, assists on the build of statistical models and segmentation, and creates reports', 'Work as a member of a Data Science & Analytics team in Mass Markets Marketing to help transform the business by identifying opportunities for operational efficiencies across the organization', 'Clearly communicating methods and conclusions with simple language and innovative visualizations to management and executives', 'Working effectively on a team and as a self-driven individual contributor who researches and develops cutting-edge and effective solutions for the business', 'Providing expertise in analyzing and visualizing large, complex, and unique data sets', 'Creating new insights that will drive business growth and profitability in the short and long term', 'Deploying analytics solutions in a continuous integration environment incorporating version control, build servers, and test servers', 'Designing and implementing programs and automation to collect, mash up, process and analyze distributed real-world data', 'The display of strong analytical thinking and skills to rapidly adopt new methods, tools, and programming languages, as needed', 'Iterate and innovate on generating novel solutions that help drive business decisions']}, 'job_onet_soc': '15111100', 'job_onet_job_zone': '5'}, {'job_id': 'z0UP1yhsJShYpIERAAAAAA==', 'job_title': 'Principal Data Scientist, Delivery Technology', 'employer_name': 'Gopuff', 'employer_logo': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcRi67EnYb_7EUlQVdVm1D9Eon0ScEZ8xj3NL4OV&s=0', 'employer_website': 'https://www.gopuff.com', 'job_publisher': 'Remote Army', 'job_employment_type': 'Full-time', 'job_employment_types': ['FULLTIME'], 'job_apply_link': 'https://remotearmy.io/jobs/DK5dGgSf-principal-data-scientist-delivery-technology?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'job_apply_is_direct': False, 'apply_options': [{'publisher': 'Remote Army', 'apply_link': 'https://remotearmy.io/jobs/DK5dGgSf-principal-data-scientist-delivery-technology?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': False}, {'publisher': 'SimplyHired', 'apply_link': 'https://www.simplyhired.com/job/NfwxeGNMvCk2LeNtGEfr2jZYLr1vTthRblhfuIOgnvGENFrg85RSVg?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': False}, {'publisher': 'Talentify', 'apply_link': 'https://www.talentify.io/job/principal-data-scientist-delivery-technology-gopuff-3e00ba50-4ffe-491e-b418-ea3144e46293?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': False}], 'job_description': 'Overview\\n\\nAre you passionate about data science? Do you describe yourself as an innovative problem solver? Looking for an opportunity that will directly impact the customer experience at Gopuff? Come join the Gopuff Data Science team and #ChangeTheGame. Gopuff is a high-growth startup that is revolutionizing hyper-local instant e-commerce. We warehouse and deliver tens of thousands of products to your door super fast.\\n\\nAs a Principal Data Scientist at Gopuff working on the Delivery Technology Team, you will lead the vision and delivery of data driven models to optimize the delivery experience for consumers and delivery partners on our platform across dispatch optimization, delivery routing/ETAs, driver pricing/incentives, as well as the delivery partner experience. All of that, with a direct impact on the delivery experience that is so core to serving our customersâ€™ immediate needs, means youâ€™ll have the opportunity to make a huge impact on the future success of Gopuff.\\n\\nWeâ€™re looking for a doer, problem solver, and thought-leader.\\nYou Will:\\nâ€¢ Collaborate with our business, product, and engineering partners across the organization to vision and deliver data science solutions for practical business problems\\nâ€¢ Deeply understand our users, our products, and our data to identify opportunities for creating machine learning, optimization and causal inference solutions to deliver a better customer experience and make our business more efficient\\nâ€¢ Build data driven, robust and replicable data science models\\nâ€¢ Use experimentation and causal inference to measure the impact of our models on user engagement and business metrics\\nâ€¢ Serve as a tech lead on the Delivery Technology DS team\\nâ€¢ Provide mentorship to junior data scientists on the team\\nYou Have:\\nâ€¢ 5+ years of industrial experiences in data science\\nâ€¢ MS or PhD in Operations Research, Mathematics, Statistics, Economics, Computer Science, Engineering or other quantitative field\\nâ€¢ Strong ability in translating ambiguous business problems into well-defined technical objectives and providing concrete data science solutions.\\nâ€¢ Expert in machine learning, statistics, and experimentation.\\nâ€¢ Proficient in Python, SQL, and source control management tools.\\nâ€¢ Excellent communication and presentation skills (to both technical and business audiences).\\nâ€¢ Effective, resilient, resourceful, and kind.\\nâ€¢ Experience in delivery tech\\nâ€¢ Experience in dispatch optimization\\nCompensation\\nâ€¢ Gopuff pays employees based on market pricing and pay may vary depending on your location. The salary range below reflects what weâ€™d reasonably expect to pay candidates. A candidateâ€™s starting pay will be determined based on job-related skills, experience, qualifications, work location, and market conditions. These ranges may be modified in the future. For additional information on this roleâ€™s compensation package, please reach out to the designated recruiter for this role.\\nâ€¢ Remote US Range: $175,000 - $250,000\\n\\nAt Gopuff, we know that life can be unpredictable. Sometimes you forget the milk at the store, run out of pet food for Fido, or just really need ice cream at 11 pm. We get itâ€”stuff happens. But thatâ€™s where we come in, delivering all your wants and needs in just minutes.\\n\\nAnd now, weâ€™re assembling a team of motivated people to help us drive forward that vision to bring a new age of convenience and predictability to an unpredictable world.\\n\\nLike what youâ€™re hearing? Then join us on Team Blue.\\n\\nGopuff is an equal employment opportunity employer, committed to an inclusive workplace where we do not discriminate on the basis of race, sex, gender, national origin, religion, sexual orientation, gender identity, marital or familial status, age, ancestry, disability, genetic information, or any other characteristic protected by applicable laws. We believe in diversity and encourage any qualified individual to apply.', 'job_is_remote': True, 'job_posted_at': '11 days ago', 'job_posted_at_timestamp': 1742688000, 'job_posted_at_datetime_utc': '2025-03-23T00:00:00.000Z', 'job_location': None, 'job_city': None, 'job_state': None, 'job_country': None, 'job_latitude': None, 'job_longitude': None, 'job_benefits': None, 'job_google_link': 'https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3Dz0UP1yhsJShYpIERAAAAAA%3D%3D&vssid=jobs-detail-viewer', 'job_min_salary': 90000, 'job_max_salary': 170000, 'job_salary_period': 'YEAR', 'job_highlights': {}, 'job_onet_soc': '15111100', 'job_onet_job_zone': '5'}, {'job_id': 'roZHcXTa6SgHOu63AAAAAA==', 'job_title': 'Senior Data Scientist, End to End Data Systems', 'employer_name': 'NVIDIA', 'employer_logo': 'https://encrypted-tbn0.gstatic.com/images?q=tbn:ANd9GcTowSmyK_56fVTyciE3AdkNaGHprjvyoZlvCWQ5&s=0', 'employer_website': None, 'job_publisher': 'JobzMall', 'job_employment_type': 'Full-time', 'job_employment_types': ['FULLTIME'], 'job_apply_link': 'https://www.jobzmall.com/nvidia/job/senior-data-scientist-end-to-end-data-systems?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'job_apply_is_direct': True, 'apply_options': [{'publisher': 'JobzMall', 'apply_link': 'https://www.jobzmall.com/nvidia/job/senior-data-scientist-end-to-end-data-systems?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic', 'is_direct': True}], 'job_description': \"We are looking for an enthusiastic and experienced Senior Data Scientist to join the End to End Data Systems team at NVIDIA. This is a unique opportunity to work at the cutting edge of data science and make an impact on the future of AI.The ideal candidate will have a strong background in data science and experience in leading large-scale data science projects. An innovative mindset and ability to work with internal and external partners is a must.To be successful in this role, you should have:â€¢ A Master's degree in data science, computer science, or a related fieldâ€¢ 5+ years of experience in data science, including machine learning, statistical modeling, and data analysisâ€¢ Proven experience in designing and developing custom data applicationsâ€¢ Expertise in Python and SQLâ€¢ Experience in using cloud-based data services (e.g. AWS, Azure, etc.)â€¢ Excellent written and verbal communication skillsIf you have a passion for data and a desire to make an impact on the future of AI, we want to hear from you!\\n\\nResponsibilities:\\n\\nâ€¢ Design and implement data science models, algorithms, and applications for large-scale data science projects\\nâ€¢ Develop custom data applications to support data-driven projects\\nâ€¢ Develop and maintain data pipelines to facilitate data collection and analysis\\nâ€¢ Analyze and interpret data to draw meaningful insights from data\\nâ€¢ Collaborate with internal and external partners to ensure the data collected is of the highest quality\\nâ€¢ Create and maintain documentation and reports related to data science projects\\nâ€¢ Utilize cutting-edge technologies to optimize data science processes\\nâ€¢ Provide guidance and mentorship to other data scientists on the team\\nâ€¢ Stay abreast of the latest trends and developments in data science and AI\\n\\nNVIDIA is an Equal Opportunity Employer. We celebrate diversity and are committed to creating an inclusive environment for all employees. We do not discriminate based upon race, religion, color, national origin, sex, sexual orientation, gender identity, age, status as a protected veteran, status as an individual with a disability, or other applicable legally protected characteristics.\", 'job_is_remote': True, 'job_posted_at': None, 'job_posted_at_timestamp': None, 'job_posted_at_datetime_utc': None, 'job_location': 'Santa Clara, CA', 'job_city': 'Santa Clara', 'job_state': 'California', 'job_country': 'US', 'job_latitude': 37.354107899999995, 'job_longitude': -121.9552356, 'job_benefits': None, 'job_google_link': 'https://www.google.com/search?q=jobs&gl=us&hl=en&udm=8#vhid=vt%3D20/docid%3DroZHcXTa6SgHOu63AAAAAA%3D%3D&vssid=jobs-detail-viewer', 'job_min_salary': 110000, 'job_max_salary': 150000, 'job_salary_period': 'YEAR', 'job_highlights': {'Qualifications': ['This is a unique opportunity to work at the cutting edge of data science and make an impact on the future of AI.The ideal candidate will have a strong background in data science and experience in leading large-scale data science projects', \"An innovative mindset and ability to work with internal and external partners is a must.To be successful in this role, you should have:â€¢ A Master's degree in data science, computer science, or a related fieldâ€¢ 5+ years of experience in data science, including machine learning, statistical modeling, and data analysisâ€¢ Proven experience in designing and developing custom data applicationsâ€¢ Expertise in Python and SQLâ€¢ Experience in using cloud-based data services (e.g\", 'AWS, Azure, etc.)â€¢ Excellent written and verbal communication skills'], 'Responsibilities': ['Design and implement data science models, algorithms, and applications for large-scale data science projects', 'Develop custom data applications to support data-driven projects', 'Develop and maintain data pipelines to facilitate data collection and analysis', 'Analyze and interpret data to draw meaningful insights from data', 'Collaborate with internal and external partners to ensure the data collected is of the highest quality', 'Create and maintain documentation and reports related to data science projects', 'Utilize cutting-edge technologies to optimize data science processes', 'Provide guidance and mentorship to other data scientists on the team', 'Stay abreast of the latest trends and developments in data science and AI']}, 'job_onet_soc': '15111100', 'job_onet_job_zone': '5'}]\n",
            "\n",
            "âœ… Total Jobs Found: 10\n",
            "\n",
            "ðŸ“Œ Data Scientist - Python (US Remote) at KnowBe4\n",
            "ðŸ“ None , None\n",
            "ðŸ”— https://edtechjobs.io/jobs/116676503-data-scientist-python-us-remote?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic \n",
            "\n",
            "ðŸ“Œ Data Scientist-12199-Remote at Shuvel Digital\n",
            "ðŸ“ None , US\n",
            "ðŸ”— https://www.monster.com/job-openings/data-scientist-12199-remote--f63c8af7-b314-428a-ae2a-b854d6c29956?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic \n",
            "\n",
            "ðŸ“Œ Data Scientist IV Main - Medicare, Medicaid, Claims, SQL, SAS, Python at Kaiser Permanente\n",
            "ðŸ“ Oakland , US\n",
            "ðŸ”— https://www.dice.com/job-detail/b7722d47-8544-4a2e-9915-f1d0ecaa402a?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic \n",
            "\n",
            "ðŸ“Œ Data Scientist - Digital and Service Operations (Work from home) - Evernorth at Cigna\n",
            "ðŸ“ None , US\n",
            "ðŸ”— https://www.karkidi.com/job-details/7244-data-scientist-digital-and-service-operations-work-from-home-evernorth-job?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic \n",
            "\n",
            "ðŸ“Œ AI Data Engineer at Aquent\n",
            "ðŸ“ None , US\n",
            "ðŸ”— https://aquent.com/find-work/200379?utm_campaign=google_jobs_apply&utm_source=google_jobs_apply&utm_medium=organic \n",
            "\n",
            "end\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "844wqWYsZIxi"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}